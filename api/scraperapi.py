# -*- coding: utf-8 -*-
"""ScraperAPI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1znNWCrzkKQAhvhFNKJTn_DMAROMhnI70
"""

def scrape_movie_posters(emotion_flag):
    """Scrape movie posters from IMDb based on user emotion input."""
    
    """## What is Scraper API?
    Scraper API is great for crawling large sites, especially e-commerce sites such as Amazon and eBay, and business directories. It can easily grab results from search engines. They use a residential IP network, which means their IPs are hard to detect and block, and their software is able to solve CAPTCHAs if they do come up.
    Let see some of the Advantages of it.
    
    - Simple dashboard to manage usage and billing
    - Free plan with 5000 requests & all features
    - 24/7 support and great customer service
    - Rotating and sticky IP sessions
    - Easy setup & proper documentation
    - Able to render JavaScript pages
    - Compatible with all the languages like Node JS, PHP, Python, etc
    
    ## Go & Sign up and get free 5000 API Calls
    [Scraper API]( https://www.scraperapi.com?fpr=spidy20)
    
    Use 'SPIDY10' to get 10% discount in your plans
    
    ## Install SDK of Scraper API
    """

    """## Requests via Scraper API SDK"""
    import random
    import re
    from bs4 import BeautifulSoup as soup
    from urllib.request import urlopen, Request
    
    
    from bs4 import BeautifulSoup
    from urllib.request import urlopen
    import io
    import PIL.Image
    from IPython.display import Image as poster
    from IPython.display import display
    
    
    from scraper_api import ScraperAPIClient
    client = ScraperAPIClient('e207aa74a723444d5f4753ccc160ad1e')
    result = client.get(url = 'http://httpbin.org/ip').text
    print(result)

    """## To use two or more parameters, simply add them to the GET request"""

    import requests

    headers = {
        'Authorization': 'Bearer e207aa74a723444d5f4753ccc160ad1e',
    }

    response = requests.get('http://httpbin.org/ip', headers=headers)
    print(response.text)

    """## Making POST/PUT Requests"""

    postResult = client.post(url = 'http://httpbin.org/anything', body = {'foo': 'bar'}).text
    putResult = client.put(url = 'http://httpbin.org/anything', body = {'foo': 'bar'}).text
    print(postResult)
    print(putResult)

    """## Render Java Scrpit"""

    result = client.get(url = 'http://httpbin.org/ip', render=True).text
    print(result)

    """## Custom Headers"""

    result = client.get(url = 'http://httpbin.org/ip', headers = {"X-MyHeader": "123"}).text
    print(result)

    """## Sessions"""

    result = client.get(url = 'http://httpbin.org/ip', session_number=123).text
    print(result)

    """## Geographic Location
    For example: to ensure your requests come from the United States, set the country_code parameter to country_code=us.
    """

    result = client.get(url = 'http://httpbin.org/ip', country_code='us').text
    print(result)

    """## Autoparse
    For select websites the API will parse all the valuable data in the HTML response and return it in JSON format. To enable this feature, simply add autoparse=true to your request and the API will parse the data for you. Currently, this feature works with Amazon, Google Search and Google Shopping.
    """

    result = client.get(url = 'https://www.amazon.com/dp/B07V1PHM66', autoparse=True).text
    print(result)

    """## To get a account usage data"""

    usage = client.account()
    print(usage)

    """## Take real-time example, we are scraping IMDB movie poster & movie info using this API.

    ## See realtime project of Movie-Recommendation system in which I am fetching all the movies poster & info using this Scraper API. [See here](http://movierecommenderknn.herokuapp.com/)
    """


    def movie_poster_fetcher(imdb_link):
    ## Call to website using SDK
        url_data = client.get(imdb_link).text
        
        ## Fetch the site data
        s_data = BeautifulSoup(url_data, 'html.parser')
        
        ## Find the tag in which Image link is stored
        imdb_dp = s_data.find("meta", property="og:image")
        
        if imdb_dp is not None:
            image_url = imdb_dp.attrs['content']

            ## TO get movie title
            movie_find = s_data.find("meta", property="og:title")
            name = movie_find.attrs['content']
            name = str(name).replace('- IMDb','')

            return {'name':name,'image_url':image_url}
        else:
            return {'name':'loading','image_url':'https://t4.ftcdn.net/jpg/03/16/15/47/360_F_316154790_pnHGQkERUumMbzAjkgQuRvDgzjAHkFaQ.jpg'}


   
    # Define IMDb URLs for different emotions based on the input flag
    if emotion_flag == 1:
        emotion_urls = {
            "Enjoyment": 'https://www.imdb.com/search/title/?title_type=feature&genres=thriller&sort=moviemeter,asc',
            "Trust": 'https://www.imdb.com/search/title/?title_type=feature&genres=western&sort=moviemeter,asc'
        }
    elif emotion_flag == 0:
        emotion_urls = {
            "Sad": 'https://www.imdb.com/search/title/?title_type=feature&genres=drama&sort=moviemeter,asc',
            "Anger": 'https://www.imdb.com/search/title/?title_type=feature&genres=family&sort=moviemeter,asc',
            "Anticipation": 'https://www.imdb.com/search/title/?title_type=feature&genres=thriller&sort=moviemeter,asc',
            "Fear": 'https://www.imdb.com/search/title/?title_type=feature&genres=sport&sort=moviemeter,asc'
        }
    else:
        print("Invalid input.")
        return []

    # Randomly select an emotion URL
    emotion = random.choice(list(emotion_urls.keys()))
    urlhere = emotion_urls[emotion]

    # Define a custom user agent
    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'

    # Create a request object with the custom user agent
    req = Request(urlhere, headers={'User-Agent': user_agent})

    # Open the URL with the request object
    uClient = urlopen(req)

    # Read the page content
    page_html = uClient.read()

    # Close the connection
    uClient.close()

    page_soup = soup(page_html, "html.parser")

    # Extract movie titles and URLs
    titles_with_urls = page_soup.find_all("a", attrs={"href": re.compile(r'\/title\/tt+\d*\/')})
    print("Number of titles found:", len(titles_with_urls))

    # Shuffle the list of titles with URLs
    random.shuffle(titles_with_urls)

    # Select a random sample of 5 titles with URLs
    selected_titles_with_urls = random.sample(titles_with_urls, 5)

    # Initialize a list to store URLs
    urls_list = []

    for title in selected_titles_with_urls:
        movie_url = 'https://www.imdb.com' + title['href']
        urls_list.append(movie_url)
        
        
    final_list=[]
    for url in urls_list:
        movie_detail =  movie_poster_fetcher(url)
        final_list.append(movie_detail)
        
    return final_list

